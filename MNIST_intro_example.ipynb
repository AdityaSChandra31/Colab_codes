{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOE/CBuH7WaWXVIFH6EWA30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaSChandra31/Colab_codes/blob/main/MNIST_intro_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XZ2BdXNFEIsZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets,transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Load MNIST Dataset\n",
        "\n",
        "train_dataset=datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_dataset=datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create DataLoaders\n",
        "\n",
        "train_loader= DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_Loader= DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1000,\n",
        "    shuffle=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "MP8ZBAqXYCsb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "l_l0SxHIZMX5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx,(data,target) in enumerate(train_loader):\n",
        "  data=data.to(device)\n",
        "  target=target.to(device)\n",
        "\n",
        "  if batch_idx==0:\n",
        "    print(f'Input shape:{data.shape}')\n",
        "    print(f'Targer shape:{target.shape}')\n",
        "    print(f'Number of training batches:{len(train_loader)}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJaoNqg0YunS",
        "outputId": "c13147cd-55af-4c98-dddd-3403d4fbcfec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:torch.Size([64, 1, 28, 28])\n",
            "Targer shape:torch.Size([64])\n",
            "Number of training batches:938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MNIST_MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten= nn.Flatten()\n",
        "    self.Linear_relu_stack= nn.Sequential(\n",
        "        nn.Linear(28*28,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.flatten(x)\n",
        "    logits=self.Linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model= MNIST_MLP().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kyXiDd2cXMS",
        "outputId": "2cfdbd48-6ff9-4ca3-bf8d-4913f160e795"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST_MLP(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (Linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criteron=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.00001)"
      ],
      "metadata": {
        "id": "fYeS7pCbfgEV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,train_loader,optimizer,criterion):\n",
        "  model.train()\n",
        "  for batch_idx,(data,target) in enumerate(train_loader):\n",
        "    data=data.to(device)\n",
        "    target=target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output=model(data)\n",
        "    loss=criterion(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%64 == 0:\n",
        "      print(f'Batch {batch_idx}/{len(train_loader)},Loss:{loss.item():.4f}')\n",
        "\n",
        "train_epoch(model,train_loader,optimizer,criteron)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP3-D7Jma_B3",
        "outputId": "7d4960b9-8608-40e9-88c3-4f10e3858b20"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/938,Loss:2.3090\n",
            "Batch 64/938,Loss:1.4816\n",
            "Batch 128/938,Loss:0.7429\n",
            "Batch 192/938,Loss:0.5779\n",
            "Batch 256/938,Loss:0.4132\n",
            "Batch 320/938,Loss:0.3300\n",
            "Batch 384/938,Loss:0.3042\n",
            "Batch 448/938,Loss:0.4025\n",
            "Batch 512/938,Loss:0.3257\n",
            "Batch 576/938,Loss:0.3013\n",
            "Batch 640/938,Loss:0.3484\n",
            "Batch 704/938,Loss:0.2416\n",
            "Batch 768/938,Loss:0.2855\n",
            "Batch 832/938,Loss:0.2281\n",
            "Batch 896/938,Loss:0.1126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet_MNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1= nn.Conv2d(1,6,5)\n",
        "    self.pool=nn.MaxPool2d(2,2)\n",
        "    self.conv2=nn.Conv2d(6,16,5)\n",
        "    self.fc1=nn.Linear(16*4*4,120)\n",
        "    self.fc2=nn.Linear(120,84)\n",
        "    self.fc3=nn.Linear(84,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.pool(F.relu(self.conv1(x)))\n",
        "    x=self.pool(F.relu(self.conv2(x)))\n",
        "    x=x.view(-1,16*4*4)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model_lenet=LeNet_MNIST().to(device)\n",
        "print(model_lenet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aetAN_pmiqZO",
        "outputId": "2c431aca-8bda-49fa-84ca-1f12531c1dbc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet_MNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_lenet=torch.optim.Adam(model_lenet.parameters(),lr=0.00001)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_epoch(model_lenet,train_loader,optimizer_lenet,criteron)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9xIHW5X3dxD",
        "outputId": "04ec01f8-5bb7-40e4-eef8-2dc8cba9604f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Batch 0/938,Loss:2.1541\n",
            "Batch 64/938,Loss:2.1164\n",
            "Batch 128/938,Loss:2.1009\n",
            "Batch 192/938,Loss:2.1120\n",
            "Batch 256/938,Loss:2.0566\n",
            "Batch 320/938,Loss:2.0896\n",
            "Batch 384/938,Loss:2.0670\n",
            "Batch 448/938,Loss:1.9846\n",
            "Batch 512/938,Loss:1.9945\n",
            "Batch 576/938,Loss:1.9893\n",
            "Batch 640/938,Loss:1.9797\n",
            "Batch 704/938,Loss:1.8214\n",
            "Batch 768/938,Loss:1.8221\n",
            "Batch 832/938,Loss:1.7441\n",
            "Batch 896/938,Loss:1.6121\n",
            "Epoch 2/10\n",
            "Batch 0/938,Loss:1.7196\n",
            "Batch 64/938,Loss:1.7765\n",
            "Batch 128/938,Loss:1.7093\n",
            "Batch 192/938,Loss:1.5264\n",
            "Batch 256/938,Loss:1.4789\n",
            "Batch 320/938,Loss:1.4357\n",
            "Batch 384/938,Loss:1.4876\n",
            "Batch 448/938,Loss:1.2587\n",
            "Batch 512/938,Loss:1.3854\n",
            "Batch 576/938,Loss:1.3392\n",
            "Batch 640/938,Loss:1.2961\n",
            "Batch 704/938,Loss:1.2339\n",
            "Batch 768/938,Loss:1.2264\n",
            "Batch 832/938,Loss:1.1954\n",
            "Batch 896/938,Loss:1.0911\n",
            "Epoch 3/10\n",
            "Batch 0/938,Loss:1.0003\n",
            "Batch 64/938,Loss:0.9584\n",
            "Batch 128/938,Loss:1.0704\n",
            "Batch 192/938,Loss:1.0171\n",
            "Batch 256/938,Loss:0.8980\n",
            "Batch 320/938,Loss:1.0177\n",
            "Batch 384/938,Loss:1.0555\n",
            "Batch 448/938,Loss:0.9377\n",
            "Batch 512/938,Loss:0.8382\n",
            "Batch 576/938,Loss:0.6571\n",
            "Batch 640/938,Loss:0.8503\n",
            "Batch 704/938,Loss:0.8205\n",
            "Batch 768/938,Loss:0.9749\n",
            "Batch 832/938,Loss:0.7500\n",
            "Batch 896/938,Loss:0.9394\n",
            "Epoch 4/10\n",
            "Batch 0/938,Loss:0.7722\n",
            "Batch 64/938,Loss:0.8142\n",
            "Batch 128/938,Loss:0.8208\n",
            "Batch 192/938,Loss:0.7104\n",
            "Batch 256/938,Loss:0.7307\n",
            "Batch 320/938,Loss:0.6515\n",
            "Batch 384/938,Loss:0.6752\n",
            "Batch 448/938,Loss:0.8170\n",
            "Batch 512/938,Loss:0.5920\n",
            "Batch 576/938,Loss:0.6758\n",
            "Batch 640/938,Loss:0.7382\n",
            "Batch 704/938,Loss:0.7087\n",
            "Batch 768/938,Loss:0.6401\n",
            "Batch 832/938,Loss:0.5598\n",
            "Batch 896/938,Loss:0.6335\n",
            "Epoch 5/10\n",
            "Batch 0/938,Loss:0.7101\n",
            "Batch 64/938,Loss:0.7867\n",
            "Batch 128/938,Loss:0.4916\n",
            "Batch 192/938,Loss:0.5544\n",
            "Batch 256/938,Loss:0.5912\n",
            "Batch 320/938,Loss:0.5314\n",
            "Batch 384/938,Loss:0.6663\n",
            "Batch 448/938,Loss:0.4487\n",
            "Batch 512/938,Loss:0.7452\n",
            "Batch 576/938,Loss:0.5006\n",
            "Batch 640/938,Loss:0.7255\n",
            "Batch 704/938,Loss:0.6058\n",
            "Batch 768/938,Loss:0.6827\n",
            "Batch 832/938,Loss:0.7019\n",
            "Batch 896/938,Loss:0.4859\n",
            "Epoch 6/10\n",
            "Batch 0/938,Loss:0.5165\n",
            "Batch 64/938,Loss:0.4595\n",
            "Batch 128/938,Loss:0.5337\n",
            "Batch 192/938,Loss:0.6710\n",
            "Batch 256/938,Loss:0.6334\n",
            "Batch 320/938,Loss:0.6935\n",
            "Batch 384/938,Loss:0.4902\n",
            "Batch 448/938,Loss:0.5500\n",
            "Batch 512/938,Loss:0.4149\n",
            "Batch 576/938,Loss:0.4079\n",
            "Batch 640/938,Loss:0.3955\n",
            "Batch 704/938,Loss:0.5039\n",
            "Batch 768/938,Loss:0.5247\n",
            "Batch 832/938,Loss:0.4067\n",
            "Batch 896/938,Loss:0.5239\n",
            "Epoch 7/10\n",
            "Batch 0/938,Loss:0.4502\n",
            "Batch 64/938,Loss:0.4524\n",
            "Batch 128/938,Loss:0.3688\n",
            "Batch 192/938,Loss:0.4466\n",
            "Batch 256/938,Loss:0.4791\n",
            "Batch 320/938,Loss:0.5004\n",
            "Batch 384/938,Loss:0.6365\n",
            "Batch 448/938,Loss:0.5726\n",
            "Batch 512/938,Loss:0.5296\n",
            "Batch 576/938,Loss:0.4446\n",
            "Batch 640/938,Loss:0.7260\n",
            "Batch 704/938,Loss:0.4190\n",
            "Batch 768/938,Loss:0.5078\n",
            "Batch 832/938,Loss:0.4359\n",
            "Batch 896/938,Loss:0.4871\n",
            "Epoch 8/10\n",
            "Batch 0/938,Loss:0.3559\n",
            "Batch 64/938,Loss:0.4220\n",
            "Batch 128/938,Loss:0.4734\n",
            "Batch 192/938,Loss:0.5388\n",
            "Batch 256/938,Loss:0.4286\n",
            "Batch 320/938,Loss:0.2820\n",
            "Batch 384/938,Loss:0.4112\n",
            "Batch 448/938,Loss:0.4589\n",
            "Batch 512/938,Loss:0.3464\n",
            "Batch 576/938,Loss:0.3333\n",
            "Batch 640/938,Loss:0.4072\n",
            "Batch 704/938,Loss:0.2369\n",
            "Batch 768/938,Loss:0.4730\n",
            "Batch 832/938,Loss:0.5333\n",
            "Batch 896/938,Loss:0.4037\n",
            "Epoch 9/10\n",
            "Batch 0/938,Loss:0.2690\n",
            "Batch 64/938,Loss:0.4664\n",
            "Batch 128/938,Loss:0.4112\n",
            "Batch 192/938,Loss:0.4620\n",
            "Batch 256/938,Loss:0.4783\n",
            "Batch 320/938,Loss:0.4113\n",
            "Batch 384/938,Loss:0.4906\n",
            "Batch 448/938,Loss:0.3272\n",
            "Batch 512/938,Loss:0.4442\n",
            "Batch 576/938,Loss:0.4710\n",
            "Batch 640/938,Loss:0.4267\n",
            "Batch 704/938,Loss:0.5728\n",
            "Batch 768/938,Loss:0.5470\n",
            "Batch 832/938,Loss:0.3284\n",
            "Batch 896/938,Loss:0.3264\n",
            "Epoch 10/10\n",
            "Batch 0/938,Loss:0.4462\n",
            "Batch 64/938,Loss:0.5652\n",
            "Batch 128/938,Loss:0.5344\n",
            "Batch 192/938,Loss:0.4413\n",
            "Batch 256/938,Loss:0.3668\n",
            "Batch 320/938,Loss:0.5088\n",
            "Batch 384/938,Loss:0.2931\n",
            "Batch 448/938,Loss:0.3452\n",
            "Batch 512/938,Loss:0.3544\n",
            "Batch 576/938,Loss:0.1553\n",
            "Batch 640/938,Loss:0.2848\n",
            "Batch 704/938,Loss:0.4932\n",
            "Batch 768/938,Loss:0.5278\n",
            "Batch 832/938,Loss:0.3422\n",
            "Batch 896/938,Loss:0.3399\n"
          ]
        }
      ]
    }
  ]
}